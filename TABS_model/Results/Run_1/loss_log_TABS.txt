================ Loss (Wed Apr 19 08:12:46 2023) ================
best_epoch: 151
train_losses: [0.12956133072192852, 0.12142082819571862, 0.11815019376002825, 0.11615446278682122, 0.11499197540374903, 0.11422708687873986, 0.11372501127994977, 0.1133621973487047, 0.11307885073698484, 0.1128509412591274, 0.11266364283286608, 0.11250253445827044, 0.11236254767729686, 0.11223998768971516, 0.11214031393711384, 0.11203085103860268, 0.1119423529276481, 0.11186555394759545, 0.11179191917181015, 0.11172255678818777, 0.11166458359131447, 0.11161676496267318, 0.11157595480863865, 0.1115407780959056, 0.11151021008308117, 0.11148347304417537, 0.11145996944262432, 0.1114394153539951, 0.11142171071125911, 0.11140727343467566, 0.11139593376563145, 0.11137794840794343, 0.11136380571585436, 0.111352232671701, 0.11134176357434346, 0.11133228517495669, 0.11132369018518008, 0.11131590948655055, 0.1113088737313564, 0.11130244548504169, 0.11129663036419796, 0.11129140613170771, 0.11128687801269385, 0.11128292668324251, 0.11127974998492461, 0.1112770975782321, 0.11127425604141676, 0.11127087703117958, 0.1112656702215855, 0.11126175923989369, 0.1112581825027099, 0.11125501440121577, 0.11125220908568456, 0.11124967267880073, 0.11124724791600155, 0.11124507257571588, 0.11124293941717882, 0.11124093544024688, 0.11123900424975615, 0.11123719146618476, 0.1112355208167663, 0.11123397109600214, 0.1112324778850262, 0.11123097584797786, 0.11122949639191994, 0.11122804501881967, 0.11122656338489972, 0.11122510513434043, 0.11122379027880155, 0.11122258305549622, 0.11122146145655559, 0.11122044313412446, 0.11121948120685723, 0.11121857223602441, 0.11121769054577901, 0.11121679429824535, 0.1112159048135464, 0.11121500455416165, 0.11121418854364983, 0.11121340176233878, 0.11121262644345943, 0.11121187129845986, 0.11121124556431404, 0.11121061765230619, 0.11121001736475872, 0.11120946888740246, 0.11120902437430162, 0.11120864542631002, 0.11120845904717079, 0.1112083695255793, 0.11120829708301104, 0.11120796215075712, 0.11120743740063448, 0.11120652074997242, 0.11120531650689933, 0.11120440902618262, 0.11120365548592348, 0.11120299433286374, 0.11120239805716735, 0.11120186322010481, 0.11120137916161464, 0.11120092043509851, 0.11120047534887607, 0.11120006430607576, 0.11119965269015386, 0.11119925677776336, 0.11119887072306413, 0.1111985098857146, 0.11119815947917791, 0.11119781984732702, 0.1111974862905649, 0.11119717072982055, 0.11119686640225925, 0.1111965679205381, 0.11119627849413799, 0.11119599651831846, 0.11119573265314102, 0.1111954757800469, 0.11119522509666589, 0.11119498002987642, 0.11119474103817573, 0.11119450766306657, 0.11119428781362681, 0.11119407392465151, 0.11119386519377049, 0.11119366162098371, 0.11119347249086087, 0.11119328221449486, 0.11119309606460424, 0.11119291656292402, 0.111192742563211, 0.1111925775041947, 0.11119242448073167, 0.11119227076952275, 0.11119212462351873, 0.11119199016919502, 0.11119185422475521, 0.11119172836725529, 0.11119160480224169, 0.11119148639532236, 0.11119137234412707, 0.11119126860912029, 0.1111911653326108, 0.11119106239997424, 0.11119096049895653, 0.11119086226591697, 0.11119076884709872, 0.11119068219111516, 0.11119060470507695, 0.11119052962614939, 0.11119045672508386, 0.11119039712043909, 0.1111903429031372, 0.11119029510479707, 0.11119025280842414, 0.11119021601401842, 0.11119018758718784, 0.1111901648915731, 0.11119014380069879, 0.11119013187976984]
val_losses: [0.12413004040718079, 0.1194667596031319, 0.11704879762096838, 0.11559340493245558, 0.11474511738527905, 0.11398058757185936, 0.11364291201938283, 0.1133534691550515, 0.11311724477193573, 0.11287539215250449, 0.11274401945146648, 0.11269468001343987, 0.11260709708387201, 0.11259350654753772, 0.11289911378513683, 0.11199816621162674, 0.11192420666868036, 0.11182439530437643, 0.11175499280745332, 0.1116968413645571, 0.11165294355966827, 0.11161362413655627, 0.11157564480196346, 0.11155025898055597, 0.11153229021213272, 0.11150946434248578, 0.11148720776492899, 0.11146062070673163, 0.1114370118487965, 0.11142248118465597, 0.1113923466341062, 0.11138751086863605, 0.11137458749792793, 0.11135964095592499, 0.11134780130603096, 0.11133798042481596, 0.11132837256247347, 0.11131882870739157, 0.11131047423590314, 0.11130322719162161, 0.11129758337681944, 0.11129378967664459, 0.11129046333107082, 0.11128675429658456, 0.11128075251525099, 0.11127720942551439, 0.11127399212934753, 0.11127146164124663, 0.1112730313431133, 0.11127538403326814, 0.11127195439555428, 0.11126854236830365, 0.11126738007773053, 0.11126643453132022, 0.11126658557490869, 0.11126669733361765, 0.11126552318984811, 0.11126413534988057, 0.11126357587901028, 0.11126277765089815, 0.11125938323411075, 0.11125338009812615, 0.1112470609897917, 0.11124182458628308, 0.11123722927136855, 0.11123437400568616, 0.11123187331990762, 0.11122947457161816, 0.11122740974480455, 0.11122562736272812, 0.11122416603294286, 0.11122304912317883, 0.1112222441218116, 0.11122165213931691, 0.11122106354344975, 0.11122043938799338, 0.11121977764097127, 0.11121900853785602, 0.11121813851323994, 0.11121720651333983, 0.11121612651781602, 0.11121584373441609, 0.11121467805721542, 0.11121382835236462, 0.11121310903267427, 0.11121253059668974, 0.11121216213161295, 0.11121199923482808, 0.11121181737292897, 0.11121178012002599, 0.11121132597327232, 0.11121003770015457, 0.1112098405984315, 0.11120990562168034, 0.11121010069142688, 0.11121035062453964, 0.11121096766807816, 0.11121107604015958, 0.11121131920001724, 0.11121125959537247, 0.11121144382791086, 0.11121156303720041, 0.11121155152266676, 0.11121182617816058, 0.1112123087725856, 0.11121259595860135, 0.11121297797018831, 0.11121264811266553, 0.11121254956180399, 0.1112119501287287, 0.11121127585118468, 0.11121138760989363, 0.11121108416806567, 0.11121070926839655, 0.11121032116088, 0.11121053417975252, 0.11121016300537369, 0.11121002889492294, 0.11120968278158795, 0.11120938374237581, 0.11120925843715668, 0.11120867729187012, 0.1112089678645134, 0.11120861904187636, 0.11120806228030812, 0.11120805076577446, 0.1112080683762377, 0.11120783334428613, 0.11120785434137691, 0.11120825735005466, 0.11120813678611409, 0.11120854250409386, 0.11120882833545859, 0.11120867492123084, 0.11120909147641876, 0.11120893772352826, 0.11120911924676462, 0.11120906539938667, 0.11120929907668721, 0.11120968921618028, 0.1112096756696701, 0.11120962995019826, 0.11120949109846895, 0.11120966517112472, 0.11120905591682954, 0.1112089834430001, 0.1112086447802457, 0.1112082681872628, 0.11120801419019699, 0.11120786145329475, 0.11120765588500282, 0.11120736836032434, 0.111207572912628, 0.1112076226960529, 0.11120770770040425, 0.11120795594020323, 0.1112080876800147, 0.11120826242999597, 0.1112080067396164, 0.11120759255506775]
train_time: 3.8410149842500685================ Loss (Sat May  6 14:02:07 2023) ================
best_epoch: 1
train_losses: [0.17654677148048695, 0.1682865424798085]
val_losses: [0.1714962822469798, 0.16593615846200424]
train_time: 0.045472286740938825================ Loss (Sat May  6 14:16:21 2023) ================
best_epoch: 4
train_losses: [0.17846276073954825, 0.17183709560438645, 0.1667092033596926, 0.16317221175792607, 0.16132080104461935]
val_losses: [0.17546855251897464, 0.1695232167840004, 0.16505160250447012, 0.16248869895935059, 0.16130398552526126]
train_time: 0.1120884542995029================ Loss (Sat May  6 14:23:31 2023) ================
best_epoch: 4
train_losses: [0.17846276247224144, 0.17183791378209756, 0.1667119521041249, 0.16317407405653664, 0.16132271359133166]
val_losses: [0.17551916539669038, 0.169570192694664, 0.16509275734424592, 0.1625260333220164, 0.16133915881315866]
train_time: 0.110305474864112